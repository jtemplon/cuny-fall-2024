{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5f89cb86-3861-4b4e-a741-4dd1ce0daabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import face_recognition\n",
    "import glob\n",
    "from PIL import Image, ImageDraw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d62415-a7a7-42a0-91de-859cc34a2f62",
   "metadata": {},
   "source": [
    "## Basic Me Encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97a181ae-d0c1-407b-bd8a-fda2b5186512",
   "metadata": {},
   "outputs": [],
   "source": [
    "me_image = face_recognition.load_image_file(\"../photos-of-me/john-templon.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4157deed-e5bf-46fa-8ae7-09f9a8bb133b",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_face_encoding = face_recognition.face_encodings(me_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9710aa1f-b6ec-425e-8de8-008bddcb9650",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.38714838e-03,  6.81708828e-02,  5.49692884e-02,  1.08011775e-02,\n",
       "       -1.27613127e-01,  2.67660618e-03,  1.15410388e-02, -9.01838243e-02,\n",
       "        3.00363004e-02, -7.13078082e-02,  1.51684627e-01, -1.31685644e-01,\n",
       "       -3.23040724e-01,  4.13233116e-02,  1.00331299e-01,  5.09286635e-02,\n",
       "       -5.05595244e-02, -1.16353422e-01, -2.47625679e-01, -1.08496614e-01,\n",
       "        2.70427130e-02,  3.45737450e-02,  3.41768004e-02, -3.48416716e-03,\n",
       "       -2.69242764e-01, -2.80077934e-01, -3.61988693e-02, -1.42545685e-01,\n",
       "        2.30755284e-02, -6.40255511e-02, -2.80763730e-02,  5.73513284e-02,\n",
       "       -1.47878870e-01, -7.59978592e-02,  3.04007977e-02,  1.10795483e-01,\n",
       "       -1.11916333e-01, -7.42870495e-02,  2.14553073e-01, -6.96870312e-03,\n",
       "       -2.20063582e-01,  3.10319066e-02,  7.50123262e-02,  2.23676533e-01,\n",
       "        1.54249191e-01,  4.81450297e-02,  5.05296513e-02, -1.26188695e-02,\n",
       "        7.67533779e-02, -2.77244270e-01,  1.10869944e-01,  7.90234804e-02,\n",
       "        1.13793910e-01,  1.54539749e-01,  1.18830264e-01, -1.95656508e-01,\n",
       "        2.89089084e-02,  1.62727103e-01, -1.58796757e-01,  8.65084082e-02,\n",
       "        8.71321261e-02, -1.43234938e-01, -4.34631035e-02, -1.07090391e-01,\n",
       "        2.60861218e-01,  8.37502629e-02, -1.18760347e-01, -2.05486566e-02,\n",
       "        1.04467146e-01, -1.26155227e-01, -8.11934993e-02,  9.18202922e-02,\n",
       "       -1.02551877e-01, -8.39907378e-02, -2.71097481e-01,  5.36039323e-02,\n",
       "        4.11067665e-01,  6.60378411e-02, -2.24707574e-01, -7.08096027e-02,\n",
       "       -3.08027025e-04, -1.83980428e-02,  8.83458033e-02,  8.41945261e-02,\n",
       "       -2.71550119e-02, -4.45664525e-02, -3.62610817e-02,  6.26584142e-02,\n",
       "        2.18650788e-01, -1.08649902e-01,  4.61359322e-03,  1.88389510e-01,\n",
       "        2.45224275e-02,  2.09224969e-02,  5.33169918e-02,  1.59391984e-02,\n",
       "       -1.58769250e-01, -6.68111518e-02, -6.55003041e-02,  4.12215516e-02,\n",
       "        9.12032202e-02, -1.38430610e-01, -3.58485878e-02,  5.09157032e-02,\n",
       "       -1.47774428e-01,  1.74819499e-01, -5.53248823e-03, -4.78641838e-02,\n",
       "        4.31293622e-02, -5.08824289e-02, -6.59609288e-02, -2.33456641e-02,\n",
       "        2.05500424e-01, -3.00324023e-01,  1.50837272e-01,  1.88390911e-01,\n",
       "       -4.77452427e-02,  9.19308066e-02,  3.75242010e-02,  5.32661900e-02,\n",
       "        8.26767087e-03,  9.60204005e-02, -2.76654840e-01, -1.02610305e-01,\n",
       "        1.23311192e-01, -3.02575380e-02, -1.35968029e-02,  3.41659635e-02])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_face_encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ecba1baa-e1d2-49e5-bdb4-a66c35f9979c",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_landmarks = face_recognition.face_landmarks(me_image)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89f1bc11-a531-4aee-8876-47a3598a9d81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chin': [(414, 289),\n",
       "  (417, 314),\n",
       "  (420, 337),\n",
       "  (425, 362),\n",
       "  (436, 387),\n",
       "  (454, 407),\n",
       "  (475, 425),\n",
       "  (497, 440),\n",
       "  (520, 442),\n",
       "  (542, 436),\n",
       "  (562, 417),\n",
       "  (579, 396),\n",
       "  (592, 375),\n",
       "  (599, 351),\n",
       "  (602, 327),\n",
       "  (602, 305),\n",
       "  (602, 283)],\n",
       " 'left_eyebrow': [(432, 280), (445, 268), (463, 263), (482, 264), (499, 270)],\n",
       " 'right_eyebrow': [(525, 269), (541, 262), (559, 260), (576, 264), (588, 276)],\n",
       " 'nose_bridge': [(514, 283), (515, 299), (516, 315), (516, 332)],\n",
       " 'nose_tip': [(495, 342), (505, 344), (516, 347), (527, 344), (536, 341)],\n",
       " 'left_eye': [(456, 290),\n",
       "  (466, 284),\n",
       "  (476, 284),\n",
       "  (486, 290),\n",
       "  (476, 292),\n",
       "  (466, 292)],\n",
       " 'right_eye': [(539, 288),\n",
       "  (549, 282),\n",
       "  (559, 281),\n",
       "  (570, 285),\n",
       "  (560, 288),\n",
       "  (549, 289)],\n",
       " 'top_lip': [(471, 366),\n",
       "  (488, 363),\n",
       "  (504, 362),\n",
       "  (516, 363),\n",
       "  (527, 361),\n",
       "  (542, 361),\n",
       "  (560, 363),\n",
       "  (554, 364),\n",
       "  (527, 366),\n",
       "  (517, 368),\n",
       "  (505, 367),\n",
       "  (477, 367)],\n",
       " 'bottom_lip': [(560, 363),\n",
       "  (544, 378),\n",
       "  (530, 386),\n",
       "  (518, 388),\n",
       "  (506, 386),\n",
       "  (489, 380),\n",
       "  (471, 366),\n",
       "  (477, 367),\n",
       "  (505, 378),\n",
       "  (517, 379),\n",
       "  (529, 377),\n",
       "  (554, 364)]}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30227168-c05f-4bf8-81a0-7a9fe338c18d",
   "metadata": {},
   "source": [
    "## Let's Locate \"Me\" In Other Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d02d65d-69bb-49c4-850a-bb2d46f3f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "other_images = [ i for i in glob.glob(\"../photos-of-me/*\") if i.split(\"/\")[-1] != \"john-templon.jpeg\" ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77a26071-d6f5-4537-8e14-d4af5ead599e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../photos-of-me/8EE554C2-2D92-44B1-B189-06C510210C99_1_105_c.jpeg',\n",
       " '../photos-of-me/5F17CDA8-28CD-46EB-844E-B236BB4E1AA7_1_105_c.jpeg',\n",
       " '../photos-of-me/8D34FBBD-333D-4058-9385-1B92D103573B_1_105_c.jpeg',\n",
       " '../photos-of-me/444E13E9-4CD1-425B-87BB-AF590B701B96_1_105_c.jpeg',\n",
       " '../photos-of-me/31548ACE-18D5-410D-9821-3A0C4FB8B424_1_105_c.jpeg',\n",
       " '../photos-of-me/784480C4-8109-41CE-9E1C-DCB82565A23B_1_105_c.jpeg',\n",
       " '../photos-of-me/8A8C5761-AEB4-4484-9EDE-258144970775_1_105_c.jpeg',\n",
       " '../photos-of-me/747990F8-B4FB-4223-B537-2176E4BA0424.jpeg',\n",
       " '../photos-of-me/0A536EA2-A6CA-422B-8AF8-288427A8EBCA_1_105_c.jpeg']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "950e3f6d-62eb-45b6-883c-2a4cd2441a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../photos-of-me/8EE554C2-2D92-44B1-B189-06C510210C99_1_105_c.jpeg\n",
      "3 face encodings found\n",
      "I'm in this photo!\n",
      "../photos-of-me/5F17CDA8-28CD-46EB-844E-B236BB4E1AA7_1_105_c.jpeg\n",
      "1 face encodings found\n",
      "I'm in this photo!\n",
      "../photos-of-me/8D34FBBD-333D-4058-9385-1B92D103573B_1_105_c.jpeg\n",
      "0 face encodings found\n",
      "../photos-of-me/444E13E9-4CD1-425B-87BB-AF590B701B96_1_105_c.jpeg\n",
      "1 face encodings found\n",
      "I'm in this photo!\n",
      "../photos-of-me/31548ACE-18D5-410D-9821-3A0C4FB8B424_1_105_c.jpeg\n",
      "1 face encodings found\n",
      "I'm in this photo!\n",
      "../photos-of-me/784480C4-8109-41CE-9E1C-DCB82565A23B_1_105_c.jpeg\n",
      "1 face encodings found\n",
      "I'm in this photo!\n",
      "../photos-of-me/8A8C5761-AEB4-4484-9EDE-258144970775_1_105_c.jpeg\n",
      "0 face encodings found\n",
      "../photos-of-me/747990F8-B4FB-4223-B537-2176E4BA0424.jpeg\n",
      "4 face encodings found\n",
      "I'm in this photo!\n",
      "../photos-of-me/0A536EA2-A6CA-422B-8AF8-288427A8EBCA_1_105_c.jpeg\n",
      "2 face encodings found\n",
      "I'm in this photo!\n"
     ]
    }
   ],
   "source": [
    "for oi in other_images:\n",
    "    print(oi)\n",
    "    unknown_picture = face_recognition.load_image_file(oi)\n",
    "    unknown_face_encodings = face_recognition.face_encodings(unknown_picture)\n",
    "    print(f'{len(unknown_face_encodings)} face encodings found')\n",
    "    for ufe in unknown_face_encodings:\n",
    "        results = face_recognition.compare_faces([my_face_encoding], ufe)\n",
    "        if results[0] == True:\n",
    "            print(\"I'm in this photo!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b5c962-a17f-4973-9031-e4365fbfd332",
   "metadata": {},
   "source": [
    "## Let's Draw To See What Faces It Saw In The New Year's Photo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "345c48ac-3fc2-4c97-8b1b-ac47f18af47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# image where it only found two faces\n",
    "image = face_recognition.load_image_file(\"../photos-of-me/0A536EA2-A6CA-422B-8AF8-288427A8EBCA_1_105_c.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "769b5034-7a8d-4e35-87a8-fbe13816e6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_landmarks_list = face_recognition.face_landmarks(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f4bc32b-e9db-4b0c-8d59-e46a86b49fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outline the eyebrows, lips, nose, and chin overdoing it at 10 to make it show up\n",
    "pil_image = Image.fromarray(image)\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    d = ImageDraw.Draw(pil_image, 'RGBA')\n",
    "\n",
    "    # Eyebrows\n",
    "    d.line(face_landmarks['left_eyebrow'], fill='black', width=10)\n",
    "    d.line(face_landmarks['right_eyebrow'], fill='black', width=10)\n",
    "\n",
    "    # Lips\n",
    "    d.line(face_landmarks['top_lip'], fill='black', width=10)\n",
    "    d.line(face_landmarks['bottom_lip'], fill='black', width=10)\n",
    "\n",
    "    # Nose\n",
    "    d.line(face_landmarks['nose_bridge'], fill='black', width=10)\n",
    "\n",
    "    # Chin\n",
    "    d.line(face_landmarks['chin'], fill='black', width=10)\n",
    "\n",
    "pil_image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b42dd2-9e45-4277-a00f-043b7d552518",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "---\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
